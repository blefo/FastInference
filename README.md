# FastInference - The Really Fast LLM Querying API Manager (OpenAi, HuggingFace, Ollama, ...)

Call all LLM APIs and get the responses very fast with a **highly robust and distributed** library.
All the LLMs providers can be used with FastInference  [OpenAI, Huggingface, VertexAI, TogetherAI, Azure, etc.]

## Features

- **High Performance**: Get high inference speed thanks to intelligent asynchronous and distributed querying.
- **Robust Error Handling**: Advanced mechanisms to handle exceptions, ensuring robust querying.
- **Ease of Use**: Simplified API designed for working with all the LLM providers: easy and fast.
- **Scalability**: Optimized for large datasets and high concurrency.

## Installation

FastLLMQuery can be installed using pip:

```bash
pip install fastinference
